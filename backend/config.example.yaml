# Prizms Backend Configuration
#
# This file configures the multi-round debate LLM tool with multi-provider support.
# Uses LiteLLM-style model_list format for industry-standard configuration.
#
# Copy this file to config.yaml and customize for your setup.

# Debate settings
debate_settings:
  # Directory where output files will be saved
  # Creates *.cot.md (chain of thought) and *.ans.md (answer) files for each personality.
  output_dir: outputs

  # Maximum number of debate rounds before forcing synthesis.
  # The debate will end earlier if consensus is reached.
  max_rounds: 3

  # Name of the consensus check personality (must be defined in personalities below)
  consensus_prompt: consensus_check

  # Name of the synthesizer personality (must be defined in personalities below)
  synthesizer_prompt: synthesizer

# LiteLLM-style model definitions
# Each entry defines a model with a friendly name and provider-specific parameters.
# The provider field specifies the backend: ollama, vllm, lm_studio, or anthropic
# The model field is the model identifier expected by that provider.
model_list:
  # Ollama models - Default URL: http://localhost:11434/v1
  - model_name: ollama-llama3
    litellm_params:
      provider: ollama
      model: llama3
      api_base: http://localhost:11434/v1
      api_key: ""  # Not needed for local Ollama

  - model_name: ollama-llama3-70b
    litellm_params:
      provider: ollama
      model: llama3:70b
      api_base: http://localhost:11434/v1
      api_key: ""

  # vLLM models - Default URL: http://localhost:8000/v1
  # Model ID is the HuggingFace-style path (e.g., org/model-name)
  - model_name: vllm-mistral
    litellm_params:
      provider: vllm
      model: mistralai/Mistral-7B-Instruct-v0.2
      api_base: http://localhost:8000/v1
      api_key: ""  # Not needed for local vLLM

  # LM Studio models - Default URL: http://localhost:1234/v1
  # Model ID is the HuggingFace-style path or LM Studio model name
  - model_name: lm-studio-qwen
    litellm_params:
      provider: lm_studio
      model: qwen/qwen3-4b
      api_base: http://localhost:1234/v1
      api_key: ""  # Not needed for local LM Studio

  # Anthropic Claude models - Cloud API (requires API key)
  # Get your API key at: https://console.anthropic.com
  # Set via environment variable or directly in config (not recommended for production)
  - model_name: claude-sonnet
    litellm_params:
      provider: anthropic
      model: claude-sonnet-4-20250514
      api_key: ${ANTHROPIC_API_KEY}  # Use environment variable

  - model_name: claude-haiku
    litellm_params:
      provider: anthropic
      model: claude-3-5-haiku-20241022
      api_key: ${ANTHROPIC_API_KEY}

  # xAI Grok models - Cloud API (requires API key)
  # Get your API key at: https://console.x.ai
  # Set via environment variable or directly in config (not recommended for production)
  - model_name: grok-3
    litellm_params:
      provider: grok
      model: grok-3
      api_key: ${XAI_API_KEY}  # Use environment variable

# Personality definitions
# Each personality references a model by model_name and points to a prompt file.
# All personalities must be explicitly listed here (no auto-discovery).
personalities:
  # Debate personalities - these participate in the multi-round debate
  - name: critic
    prompt: prompts/critic.txt
    model_name: ollama-llama3

  - name: judge
    prompt: prompts/judge.txt
    model_name: ollama-llama3

  - name: chaos_monkey
    prompt: prompts/chaos_monkey.txt
    model_name: ollama-llama3

  # System personalities - these are used for consensus checking and synthesis
  - name: consensus_check
    prompt: prompts/consensus_check.txt
    model_name: ollama-llama3

  - name: synthesizer
    prompt: prompts/synthesizer.txt
    model_name: ollama-llama3
